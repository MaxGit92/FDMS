{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "#FDMS tme4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP4 : Implémentation des modèles de CF avec descente de gradient stochastique\n",
    "    \n",
    "    Modèle classique L2 (sans Biais)\n",
    "    Modèle avec biais utilisateur et items\n",
    "    Evaluation des modèles sur la base MovieLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Loading Movie Lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "def loadMovieLens(path='ml-100k/'):\n",
    "    #Get movie titles\n",
    "    movies={}\n",
    "    for line in open(path+'/u.item'):\n",
    "        (id,title)=line.split('|')[0:2]\n",
    "        movies[id]=title\n",
    "\n",
    "    # Load data\n",
    "    prefs={}\n",
    "    for line in open(path+'/u.data'):\n",
    "        (user,movieid,rating,ts)=line.split('\\t')\n",
    "        prefs.setdefault(user,{})\n",
    "        prefs[user][movies[movieid]]=float(rating)\n",
    "    return prefs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Transform Data & split sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On convertit les données en liste de tuples pour faciliter le split entre test et train.\n",
    "\n",
    "On split aléatoirement sans remise avec 80% en train et 20% en test.\n",
    "\n",
    "On transforme les ratings dans une matrice Users*Items pour pouvoir faire des calculs matriciels par la suite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now reindexing everything\n",
    "#flatten the whole thing\n",
    "def flatten_dict(ratings):\n",
    "    tuples_items = []\n",
    "    for userId, userMovies in ratings.iteritems() :\n",
    "        for movieId, rating in userMovies.iteritems() :\n",
    "            tuples_items.append((userId,movieId, rating))\n",
    "    return np.array(tuples_items)\n",
    "\n",
    "def split_data(a_ratings, perc):\n",
    "    indexes = np.random.permutation(range(len(a_ratings)))\n",
    "    seuil = perc * len(a_ratings)\n",
    "    return a_ratings[indexes[:seuil],:], a_ratings[indexes[seuil:],:]\n",
    "    \n",
    "def filter_test_rating(train_ratings, test_ratings):\n",
    "    #we want the test set to contain only films and users present in the train set\n",
    "    users = {}\n",
    "    items = {}\n",
    "                                    \n",
    "    for i in range(len(train_ratings)):\n",
    "        users[train_ratings[i,0]] = True\n",
    "        items[train_ratings[i,1]] = True\n",
    "\n",
    "    filtered_test = []\n",
    "    for i in range(len(test_ratings)):\n",
    "        try :\n",
    "            users[test_ratings[i,0]]\n",
    "            items[test_ratings[i,1]]\n",
    "            \n",
    "            filtered_test.append(test_ratings[i])\n",
    "        except KeyError:\n",
    "            pass # do not add\n",
    "    return np.array(filtered_test)\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "#build only indexes from the train ratings because we assume the test set contains the same keys...\n",
    "def build_indexes(train_ratings):\n",
    "    idx_u = {}\n",
    "    idx_i = {}\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    for ind_l in range(len(train_ratings)) :\n",
    "        user_id, movie_id, rating = train_ratings[ind_l,:]\n",
    "        try :\n",
    "            idx_u[user_id]\n",
    "        except KeyError :\n",
    "            idx_u[user_id] = i\n",
    "            i += 1\n",
    "        try:\n",
    "            idx_i[movie_id]\n",
    "        except KeyError:\n",
    "            idx_i[movie_id] = j\n",
    "            j += 1\n",
    "    return idx_u, idx_i\n",
    "\"\"\"CSR or CSC should be your preferred option, as most functions are built for these types.\n",
    "BSR is a block version of CSR. COO and DOK are convenient for data entry, but once you have it \n",
    "all entered, you'll want to convert it to one of the other types\"\"\"\n",
    "def build_matrix(ratings, idx_u, idx_i):\n",
    "    #build matrix\n",
    "    m = np.zeros((len(idx_u), len(idx_i)))#non sparse\n",
    "    for i in range(len(ratings)) :\n",
    "        user_id, movie_id, rating = ratings[i,:]\n",
    "        m[idx_u[user_id], idx_i[movie_id]] = rating\n",
    "    return  m#spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbTrain 79754\n",
      "nbTest 19939\n",
      "nbTestFiltered 19912\n",
      "nbUsers 943\n",
      "nbMovies 1638\n",
      "----\n",
      "nbRatingsTrain 79754\n",
      "nbRatingsTestFilt 19912\n"
     ]
    }
   ],
   "source": [
    "prefs = loadMovieLens()\n",
    "\n",
    "a_ratings = flatten_dict(prefs)\n",
    "train_ratings, test_ratings = split_data(a_ratings, 0.8)\n",
    "filt_test_ratings = filter_test_rating(train_ratings, test_ratings)\n",
    "\n",
    "print 'nbTrain', len(train_ratings)\n",
    "print 'nbTest',len(test_ratings)\n",
    "print 'nbTestFiltered',len(filt_test_ratings)\n",
    "\n",
    "\n",
    "idx_u, idx_i = build_indexes(train_ratings)\n",
    "print 'nbUsers',len(idx_u)\n",
    "print 'nbMovies',len(idx_i)\n",
    "\n",
    "print '----'\n",
    "#il faut que les deux matrices soient construites avec les mêmes indexes...\n",
    "train_m = build_matrix(train_ratings, idx_u, idx_i)\n",
    "test_m = build_matrix(filt_test_ratings, idx_u, idx_i)\n",
    "print 'nbRatingsTrain',np.count_nonzero(train_m)\n",
    "print 'nbRatingsTestFilt',np.count_nonzero(test_m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Estimateurs : Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les 2 baselines testées correspondenet à la note moyenne donnée par utilisateur et par film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    def __init__(self,idx_u, idx_i):\n",
    "        self.idx_u = idx_u\n",
    "        self.idx_i = idx_i\n",
    "    def fit(self, m):\n",
    "        pass\n",
    "    def predict(self, m):\n",
    "        pass\n",
    "    def score(self, m):\n",
    "        nzero_ind = np.nonzero(m)\n",
    "        preds = self.predict(m)   \n",
    "        #il est necessaire de ne regarder que les indices non egaux a zero\n",
    "        #sinon on va regarder des predictions qui ne nous interessent pas...\n",
    "        #on suppose dans m que les cases non à zero sont celles qui nous interessent\n",
    "        #x'Est couteux en memoire mais bon... c'Est plus facile pour les operations\n",
    "        dif_abs = np.power(np.subtract(m[nzero_ind],preds[nzero_ind]),2)\n",
    "        return dif_abs.mean()\n",
    "    \n",
    "#le mean est un peu dégueu mais faut qu on le fasse sur tout ce qui est à pas zero...\n",
    "class BaselineMeanUser(Estimator):\n",
    "    def __init__(self, idx_u, idx_i):\n",
    "        Estimator.__init__(self,idx_u, idx_i)\n",
    "    def fit(self, train_m):#on suppose que les indixes des ibjets sont dans k'index avec lequel on initialise...\n",
    "        self.mean_by_user = [train_m[i,:][np.nonzero(train_m[i,:])].mean() for i in range(train_m.shape[0])]\n",
    "        return np.array(self.mean_by_user)\n",
    "    def predict(self, test_m):\n",
    "        preds = np.zeros((len(self.idx_u), len(self.idx_i)))\n",
    "        preds = preds.T + self.mean_by_user #on met sur les lignes !Attenton broadcast\n",
    "        return preds.T\n",
    "    \n",
    "class BaselineMeanMovie(Estimator):\n",
    "    def __init__(self, index_users, index_items):\n",
    "        Estimator.__init__(self,index_users,index_items)\n",
    "    def fit(self, train_m):\n",
    "        self.mean_by_movie = [train_m[:,j][np.nonzero(train_m[:,j])].mean() for j in range(train_m.shape[1])]\n",
    "        return np.array(self.mean_by_movie)\n",
    "    def predict(self, test_m):\n",
    "        preds = np.zeros((len(self.idx_u), len(self.idx_i)))\n",
    "        preds = preds + self.mean_by_movie #on met sur les colonnes\n",
    "        return preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SquaredDistance train BLMeanUser 1.05847516135\n",
      "SquaredDistance test BLMeanUser 1.09249901866\n",
      "SquaredDistance train BLMeanMovie 0.994436913073\n",
      "SquaredDistance test BLMeanMovie 1.05049164588\n"
     ]
    }
   ],
   "source": [
    "estimateur = BaselineMeanUser(idx_u,idx_i)\n",
    "estimateur.fit(train_m)\n",
    "score = estimateur.score(train_m)\n",
    "print 'SquaredDistance train BLMeanUser',score\n",
    "score = estimateur.score(test_m)\n",
    "print 'SquaredDistance test BLMeanUser',score\n",
    "\n",
    "estimateur = BaselineMeanMovie(idx_u,idx_i)\n",
    "estimateur.fit(train_m)\n",
    "score = estimateur.score(train_m)\n",
    "print 'SquaredDistance train BLMeanMovie',score\n",
    "score = estimateur.score(test_m)\n",
    "print 'SquaredDistance test BLMeanMovie',score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score calculé correspond à la moyenne des erreurs au carré.\n",
    "\n",
    "La baseline MeanMovie est plus pertinente que la baseline MeanUser.\n",
    "\n",
    "La distance est plus grande pour les données de test que d'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Modèle Classique L2 (Sans Biais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MatrixFactorizationEstimator(object):\n",
    "    def __init__(self, lamb,eps=1e-4,K=3,I = 10,doprint = False,itprint = 10 ):\n",
    "        self.eps = eps\n",
    "        self.K = K\n",
    "        self.I = I\n",
    "        self.lamb = lamb\n",
    "        self.itprint = itprint\n",
    "        self.doprint = doprint\n",
    "    def initialize(self,R):\n",
    "        self.P = np.random.rand(R.shape[0],self.K)#utilisateurs\n",
    "        self.Q = np.random.rand(self.K,R.shape[1]) #items\n",
    "        self.log_loss=[]\n",
    "        self.log_score=[]\n",
    "    def fit(self, R):\n",
    "        self.initialize(R)\n",
    "        ind = np.nonzero(R)\n",
    "        nb_ex = len(ind[0])\n",
    "        for it in xrange(self.I):         \n",
    "            #recupere les indices des exemples\n",
    "            loss = 0.\n",
    "            for stoch in xrange(nb_ex):#gradient stochastique\n",
    "                rand_ind = np.random.randint(nb_ex)\n",
    "                i, j = ind[0][rand_ind], ind[1][rand_ind]\n",
    "               \n",
    "                rij = np.dot(self.P[i,:].reshape((1, self.K)), self.Q[:,j].reshape((self.K, 1)))\n",
    "                eij = R[i,j] - rij\n",
    "               \n",
    "                temp_P = (1 - self.lamb * self.eps) * self.P[i,:] + 2. * self.eps * eij * self.Q[:,j]#.reshape()\n",
    "                self.Q[:,j] = (1 - self.lamb * self.eps) * self.Q[:,j] + 2. * self.eps * eij * self.P[i,:]#.reshape()\n",
    "                self.P[i,:] = temp_P#maj simultanée\n",
    "               \n",
    "                loss += eij**2 #loss = self.loss(R)\n",
    "               \n",
    "            self.log_loss.append(loss/nb_ex)\n",
    "            if it % self.itprint==0:               \n",
    "                if self.doprint :\n",
    "                    print '\\nit = ', it\n",
    "                    print 'loss : ', np.mean(self.log_loss[-self.itprint:])#acc_loss / self.itprint\n",
    "                    score = self.score(R)\n",
    "                    self.log_score.append(score)\n",
    "                    print 'score ',score\n",
    "                    print 'true loss', self.loss(R)\n",
    "    def predict(self, test_m):\n",
    "        return np.dot(self.P, self.Q)\n",
    "    def score(self, m):\n",
    "        return ((m[np.nonzero(m)] - self.predict(m)[np.nonzero(m)] )**2).mean()\n",
    "    def loss(self, m):\n",
    "        return ((m[np.nonzero(m)] - self.predict(m)[np.nonzero(m)] )**2).mean().sum() + self.lamb*(np.power(self.P,2).sum() + np.power(self.Q,2).sum())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it =  0\n",
      "loss :  2.804335849\n",
      "score  2.78812887946\n",
      "true loss 1725.80994686\n",
      "\n",
      "it =  50\n",
      "loss :  2.11406268868\n",
      "score  1.65771515719\n",
      "true loss 1811.55511081\n",
      "\n",
      "it =  100\n",
      "loss :  1.44558394126\n",
      "score  1.29263841292\n",
      "true loss 1856.94608516\n",
      "\n",
      "it =  150\n",
      "loss :  1.21050757422\n",
      "score  1.1427402866\n",
      "true loss 1879.60038379\n",
      "\n",
      "it =  200\n",
      "loss :  1.09980293351\n",
      "score  1.06378688521\n",
      "true loss 1891.97087117\n",
      "\n",
      "it =  250\n",
      "loss :  1.03651766734\n",
      "score  1.01506629228\n",
      "true loss 1899.24023036\n",
      "\n",
      "it =  300\n",
      "loss :  0.996617963594\n",
      "score  0.981794379517\n",
      "true loss 1903.75555892\n",
      "\n",
      "it =  350\n",
      "loss :  0.969384606074\n",
      "score  0.957530304814\n",
      "true loss 1906.73152772\n",
      "\n",
      "it =  400\n",
      "loss :  0.948284934776\n",
      "score  0.939097707057\n",
      "true loss 1908.56151577\n",
      "\n",
      "it =  450\n",
      "loss :  0.932413987343\n",
      "score  0.92455273674\n",
      "true loss 1909.57415914\n",
      "\n",
      "it =  500\n",
      "loss :  0.918440472535\n",
      "score  0.912714122974\n",
      "true loss 1910.22253935\n",
      "\n",
      "it =  550\n",
      "loss :  0.907143123562\n",
      "score  0.903127487072\n",
      "true loss 1910.22240026\n",
      "\n",
      "it =  600\n",
      "loss :  0.899585872914\n",
      "score  0.895022014217\n",
      "true loss 1910.03055374\n",
      "\n",
      "it =  650\n",
      "loss :  0.890972137641\n",
      "score  0.888145527668\n",
      "true loss 1909.67586053\n",
      "\n",
      "it =  700\n",
      "loss :  0.884520345763\n",
      "score  0.882057610383\n",
      "true loss 1909.24532415\n",
      "\n",
      "it =  750\n",
      "loss :  0.879516252214\n",
      "score  0.876748122817\n",
      "true loss 1908.7442495\n",
      "\n",
      "it =  800\n",
      "loss :  0.874149587408\n",
      "score  0.872200547687\n",
      "true loss 1907.97930331\n",
      "\n",
      "it =  850\n",
      "loss :  0.871993411306\n",
      "score  0.868126452411\n",
      "true loss 1907.18265328\n",
      "\n",
      "it =  900\n",
      "loss :  0.865213252171\n",
      "score  0.864608677596\n",
      "true loss 1906.20316033\n",
      "\n",
      "it =  950\n",
      "loss :  0.862369594763\n",
      "score  0.861309987556\n",
      "true loss 1905.39721639\n",
      "score BIAS, train :  0.858499043254\n",
      "score BIAS, test :  0.923982485306\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outfile = \"data_exp\"\n",
    "estimateur1 = MatrixFactorizationEstimator( lamb=0.2,eps=1e-5,K=10,I = 1200,doprint = True,itprint = 50)\n",
    "estimateur1.fit(train_m)\n",
    "score_train = estimateur1.score(train_m)\n",
    "print 'score BIAS, train : ',score_train\n",
    "score_test = estimateur1.score(test_m)\n",
    "print 'score BIAS, test : ',score_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme met du temps à converger (1200 époques), sûrement du au fait que nous ayons codé l'algorithme avec des matrices normales. Pour atteindre un score un peu meilleur que les baslines, l'algorithme a tourné à peu près 20 mn.\n",
    "\n",
    "On observe un surapprentissage assez marqué  dans la différence de score entre train et test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Modèle classique L2  (Avec biais utilisateur et film)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce modèle on rajoute un biais moyen et un biais utilisateur et film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MatrixFactorizationEstimatorWithBias(object):\n",
    "    def __init__(self,lamb=0,eps=1e-4,K=3,I = 10,doprint = False,itprint = 10 ):\n",
    "        self.eps = eps\n",
    "        self.K = K\n",
    "        self.I = I\n",
    "        self.lamb = lamb\n",
    "        self.itprint = itprint\n",
    "        self.doprint = doprint\n",
    "    def initialize(self,R):\n",
    "        self.P = np.random.rand(R.shape[0],self.K)#utilisateurs\n",
    "        self.Q = np.random.rand(R.shape[1],self.K).T #items\n",
    "        self.log_loss=[]\n",
    "        self.log_score=[]\n",
    "    def fit(self, R):\n",
    "        self.initialize(R)\n",
    "       \n",
    "        #calcule biais initiaux\n",
    "        self.mu = R[np.nonzero(R)].mean()\n",
    "        self.B_u = np.array([R[i,:][np.nonzero(R[i,:])].mean() for i in range(R.shape[0])]) - self.mu\n",
    "        self.B_i= np.array([R[:,j][np.nonzero(R[:,j])].mean() for j in range(R.shape[1])]) - self.mu\n",
    "       \n",
    "        ind = np.nonzero(R)\n",
    "        nb_ex = len(ind[0])\n",
    "        for it in xrange(self.I): \n",
    "            #recupere les indices des exemples\n",
    "            loss= 0.\n",
    "            for stoch in xrange(nb_ex):#gradient stochastique\n",
    "                rand_ind = np.random.randint(nb_ex)\n",
    "                i, j = ind[0][rand_ind], ind[1][rand_ind]\n",
    "               \n",
    "                rij = self.mu + self.B_u[i] + self.B_i[j] + np.dot(self.P[i,:], self.Q[:,j])\n",
    "                eij = R[i,j] - rij               \n",
    "               \n",
    "                temp_P = (1 - self.lamb * self.eps) * self.P[i,:] + 2 * self.eps * eij * self.Q[:,j]\n",
    "                self.Q[:,j] = (1 - self.lamb * self.eps) * self.Q[:,j] + 2 * self.eps * eij * self.P[i,:]\n",
    "                self.P[i,:] = temp_P#maj simultanée\n",
    "   \n",
    "                self.B_u[i] = (1 - self.lamb * self.eps) * self.B_u[i] + 2 * self.eps * eij\n",
    "                self.B_i[j] = (1 - self.lamb * self.eps) * self.B_i[j] + 2 * self.eps * eij\n",
    "                self.mu = (1 - self.lamb * self.eps) * self.mu + 2 * self.eps * eij\n",
    "               \n",
    "                loss += eij**2\n",
    "           \n",
    "            self.log_loss.append(loss/nb_ex)\n",
    "            if it % self.itprint==0:\n",
    "                if self.doprint :\n",
    "                    #print '\\nit = ', it\n",
    "                    print 'loss : ', np.mean(self.log_loss[:-self.itprint])\n",
    "                    score = self.score(R)\n",
    "                    self.log_score.append(score)\n",
    "                    print 'score ',score\n",
    "                    print 'true loss', self.loss(R)\n",
    "  \n",
    "    def predict(self, test_m):       \n",
    "        return ((self.mu + np.dot(self.P, self.Q)).T + self.B_u).T  + self.B_i\n",
    "    def score(self, m):\n",
    "        return ((m[np.nonzero(m)] - self.predict(m)[np.nonzero(m)] )**2).mean()\n",
    "    def loss(self, R):\n",
    "        return np.power(R[np.nonzero(R)] - self.predict(R)[np.nonzero(R)], 2).sum() + self.lamb*((self.P**2).sum() + (self.Q**2).sum() + (self.B_u**2).sum() + (self.B_i**2).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : nan\n",
      "score  1.44356364286\n",
      "true loss 117153.620756\n",
      "loss :  2.94672604467\n",
      "score  1.07754102464\n",
      "true loss 87932.1923028\n",
      "loss :  1.21396883783\n",
      "score  0.994377206742\n",
      "true loss 81280.6899857\n",
      "loss :  1.12263422749\n",
      "score  0.953114539313\n",
      "true loss 77975.1265255\n",
      "loss :  1.07267448648\n",
      "score  0.926705224171\n",
      "true loss 75856.5192978\n",
      "loss :  1.03945525739\n",
      "score  0.908775342635\n",
      "true loss 74415.6348851\n",
      "loss :  1.0154365205\n",
      "score  0.89773566825\n",
      "true loss 73525.2919627\n",
      "loss :  0.996997432402\n",
      "score  0.887660103129\n",
      "true loss 72712.6812783\n",
      "loss :  0.982119739726\n",
      "score  0.878843870663\n",
      "true loss 72001.1312512\n",
      "loss :  0.969806797434\n",
      "score  0.87283657827\n",
      "true loss 71514.0953908\n",
      "loss :  0.959498349956\n",
      "score  0.867133261049\n",
      "true loss 71051.7586772\n",
      "loss :  0.950417205275\n",
      "score  0.862585000062\n",
      "true loss 70681.8937087\n",
      "loss :  0.942580958629\n",
      "score  0.859542186122\n",
      "true loss 70432.4233881\n",
      "loss :  0.935670097367\n",
      "score  0.855036277234\n",
      "true loss 70066.5462117\n",
      "loss :  0.929511245533\n",
      "score  0.851079545799\n",
      "true loss 69744.7406167\n",
      "loss :  0.924059536116\n",
      "score  0.847497601421\n",
      "true loss 69453.0410273\n",
      "loss :  0.919077833001\n",
      "score  0.845298415876\n",
      "true loss 69271.7989903\n",
      "loss :  0.914539173371\n",
      "score  0.844013325302\n",
      "true loss 69163.6317659\n",
      "loss :  0.910390228808\n",
      "score  0.84080624545\n",
      "true loss 68902.3965719\n",
      "loss :  0.906620694887\n",
      "score  0.839054672992\n",
      "true loss 68757.3765922\n",
      "score BIAS, train :  0.837292102241\n",
      "score BIAS, test :  0.912765264516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/magma/bin/anaconda/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "/home/magma/bin/anaconda/lib/python2.7/site-packages/numpy/core/_methods.py:71: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "estimateur2 = MatrixFactorizationEstimatorWithBias(lamb=0.2,eps=1e-5,K=10,I = 1000,doprint = True,itprint = 50)\n",
    "estimateur2.fit(train_m)\n",
    "score_train = estimateur2.score(train_m)\n",
    "print 'score BIAS, train : ',score_train\n",
    "score_test = estimateur2.score(test_m)\n",
    "print 'score BIAS, test : ',score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec 200 époques de moins, l'algorithme avec biais permet de converger plus rapidement à une meilleure solution.\n",
    "\n",
    "Par contre le surapprentissage est toujours marqué, le modèle plus complexe permet donc de plus coller aux données de manières dangereuses. La généralisation n'est pas forcément évidente.\n",
    "\n",
    "Il faudrait conduire une étude expérimentale plus poussée pour déterminer les meilleures valeurs de coefficients de régularisation ainsi que des pas de gradients.\n",
    "\n",
    "(mais les calculs sont assez coûteux)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sauve le modèle pour la visualisation de données avec t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('P_bias',estimateur2.P)\n",
    "np.save('Q_bias',estimateur2.Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
